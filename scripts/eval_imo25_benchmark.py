"""
Evaluation script for IMO 2025 problems using OptiLLM approaches
Designed to test MARS and other approaches on challenging proof-based problems
"""

import argparse
import json
import os
import logging
import re
import time
from typing import List, Dict, Tuple, Optional
from datetime import datetime
from openai import OpenAI
from tqdm import tqdm

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize OpenAI client for local OptiLLM server
client = OpenAI(api_key="optillm", base_url="http://localhost:8001/v1")

# Import the actual IMO 2025 problems and reference solutions
from imo25_reference import IMO_2025_PROBLEMS, verify_answer_format, verify_key_insights

SYSTEM_PROMPT = '''You are solving IMO (International Mathematical Olympiad) problems - the most challenging mathematical competition problems for high school students.

Key requirements:
1. **Complete proofs**: Provide rigorous, step-by-step mathematical proofs
2. **Mathematical rigor**: Every step must be logically justified
3. **Clear structure**: Organize your solution with clear logical flow
4. **Proper notation**: Use correct mathematical notation and formatting
5. **Verification**: Double-check your reasoning and conclusions

For existence problems: Provide explicit constructions or proofs of non-existence
For optimization problems: Prove that your answer is optimal
For functional equations: Consider injectivity, surjectivity, and special values
For geometry: Use coordinate systems, trigonometry, or synthetic methods as appropriate
For number theory: Apply divisibility, modular arithmetic, and prime factorization
For combinatorics: Use counting techniques, pigeonhole principle, and extremal arguments

Always conclude with a clear statement of your final answer.

For problems with specific answers, put your final answer in \boxed{} format.'''

def extract_final_answer(solution: str, problem_id: int) -> Dict[str, any]:
    """
    Extract and verify the final answer using official IMO 2025 solutions
    """
    # Use the official answer verification from our reference module
    official_verification = verify_answer_format(problem_id, solution)

    # Legacy extraction for fallback
    result = {
        "extracted_answer": None,
        "confidence": 0.0,
        "extraction_method": None,
        "official_answer_found": official_verification["correct_answer_found"],
        "official_answer_score": official_verification["answer_score"]
    }

    if not solution:
        return result

    # If official answer was found, prioritize it
    if official_verification["correct_answer_found"]:
        result["extracted_answer"] = official_verification["extracted_answer"]
        result["confidence"] = 1.0
        result["extraction_method"] = "official_verification"
        return result

    # Look for boxed answers first
    boxed_pattern = r'\\boxed\{([^}]+)\}'
    boxed_matches = re.findall(boxed_pattern, solution)
    if boxed_matches:
        result["extracted_answer"] = boxed_matches[-1].strip()  # Take the last one
        result["confidence"] = 0.9
        result["extraction_method"] = "boxed"
        return result

    # Look for "final answer" or "answer:" sections
    answer_patterns = [
        r'final answer[:\s]*([^\n]+)',
        r'answer[:\s]*([^\n]+)',
        r'therefore[:\s]*([^\n]+)',
        r'thus[:\s]*([^\n]+)'
    ]

    solution_lower = solution.lower()
    for pattern in answer_patterns:
        matches = re.findall(pattern, solution_lower)
        if matches:
            result["extracted_answer"] = matches[-1].strip()
            result["confidence"] = 0.5
            result["extraction_method"] = "answer_section"
            break

    return result


def imo25_verify_solution(problem: str, solution: str, model: str) -> Dict[str, any]:
    """
    Two-stage verification system from IMO25 repository:
    Stage 1: Detailed verification using comprehensive IMO grader prompt
    Stage 2: Simple yes/no check on solution correctness
    """

    # Stage 1: Detailed verification using IMO25's verification system prompt
    verification_system_prompt = """You are an expert mathematician and a meticulous grader for an International Mathematical Olympiad (IMO) level exam. Your primary task is to rigorously verify the provided mathematical solution. A solution is to be judged correct **only if every step is rigorously justified.** A solution that arrives at a correct final answer through flawed reasoning, educated guesses, or with gaps in its arguments must be flagged as incorrect or incomplete.

### Instructions ###

**1. Core Instructions**
*   Your sole task is to find and report all issues in the provided solution. You must act as a **verifier**, NOT a solver. **Do NOT attempt to correct the errors or fill the gaps you find.**
*   You must perform a **step-by-step** check of the entire solution. This analysis will be presented in a **Detailed Verification Log**, where you justify your assessment of each step: for correct steps, a brief justification suffices; for steps with errors or gaps, you must provide a detailed explanation.

**2. How to Handle Issues in the Solution**
When you identify an issue in a step, you MUST first classify it into one of the following two categories and then follow the specified procedure.

*   **a. Critical Error:**
    This is any error that breaks the logical chain of the proof. This includes both **logical fallacies** (e.g., claiming that `A>B, C>D` implies `A-C>B-D`) and **factual errors** (e.g., a calculation error like `2+3=6`).
    *   **Procedure:**
        *   Explain the specific error and state that it **invalidates the current line of reasoning**.
        *   Do NOT check any further steps that rely on this error.
        *   You MUST, however, scan the rest of the solution to identify and verify any fully independent parts. For example, if a proof is split into multiple cases, an error in one case does not prevent you from checking the other cases.

*   **b. Justification Gap:**
    This is for steps where the conclusion may be correct, but the provided argument is incomplete, hand-wavy, or lacks sufficient rigor.
    *   **Procedure:**
        *   Explain the gap in the justification.
        *   State that you will **assume the step's conclusion is true** for the sake of argument.
        *   Then, proceed to verify all subsequent steps to check if the remainder of the argument is sound.

**3. Output Format**
Your response MUST be structured into two main sections: a **Summary** followed by the **Detailed Verification Log**.

*   **a. Summary**
    This section MUST be at the very beginning of your response. It must contain two components:
    *   **Final Verdict**: A single, clear sentence declaring the overall validity of the solution. For example: "The solution is correct," "The solution contains a Critical Error and is therefore invalid," or "The solution's approach is viable but contains several Justification Gaps."
    *   **List of Findings**: A bulleted list that summarizes **every** issue you discovered. For each finding, you must provide:
        *   **Location:** A direct quote of the key phrase or equation where the issue occurs.
        *   **Issue:** A brief description of the problem and its classification (**Critical Error** or **Justification Gap**).

*   **b. Detailed Verification Log**
    Following the summary, provide the full, step-by-step verification log as defined in the Core Instructions. When you refer to a specific part of the solution, **quote the relevant text** to make your reference clear before providing your detailed analysis of that part.

**Example of the Required Summary Format**
*This is a generic example to illustrate the required format. Your findings must be based on the actual solution provided below.*

**Final Verdict:** The solution is **invalid** because it contains a Critical Error.

**List of Findings:**
*   **Location:** "By interchanging the limit and the integral, we get..."
    *   **Issue:** Justification Gap - The solution interchanges a limit and an integral without providing justification, such as proving uniform convergence.
*   **Location:** "From $A > B$ and $C > D$, it follows that $A-C > B-D$"
    *   **Issue:** Critical Error - This step is a logical fallacy. Subtracting inequalities in this manner is not a valid mathematical operation.

### Verification Task Reminder ###

Your task is to act as an IMO grader. Now, generate the **summary** and the **step-by-step verification log** for the solution above. In your log, justify each correct step and explain in detail any errors or justification gaps you find, as specified in the instructions above."""

    verification_prompt = f"""
======================================================================
### Problem ###

{problem}

======================================================================
### Solution ###

{solution}

{verification_system_prompt}
"""

    try:
        # Stage 1: Detailed verification
        response = client.with_options(timeout=300).chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": verification_system_prompt},
                {"role": "user", "content": verification_prompt}
            ],
            max_tokens=30000,
            temperature=0.1
        )

        verification_response = response.choices[0].message.content.strip()

        # Stage 2: Simple yes/no check on correctness
        check_correctness_prompt = f"""Response in "yes" or "no". Is the following statement saying the solution is correct, or does not contain critical error or a major justification gap?

{verification_response}"""

        response2 = client.with_options(timeout=300).chat.completions.create(
            model=model,
            messages=[
                {"role": "user", "content": check_correctness_prompt}
            ],
            max_tokens=10,
            temperature=0.1
        )

        correctness_check = response2.choices[0].message.content.strip().lower()
        is_correct = "yes" in correctness_check

        # Extract bug report if solution is incorrect
        bug_report = ""
        if not is_correct:
            # Try to extract the detailed verification log
            verification_log_match = re.search(r'### Detailed Verification Log ###\s*(.*)', verification_response, re.DOTALL)
            if verification_log_match:
                bug_report = verification_log_match.group(1).strip()
            else:
                bug_report = verification_response

        return {
            "judge_response": verification_response,
            "correctness_check": correctness_check,
            "is_correct": is_correct,
            "bug_report": bug_report,
            "correctness_score": 1.0 if is_correct else 0.0,
            "completeness_score": 1.0 if is_correct else 0.0,
            "has_key_insights": is_correct,
            "errors_found": [bug_report] if bug_report else [],
            "overall_assessment": "correct" if is_correct else "incorrect",
            "judge_reasoning": verification_response,
            "success": True
        }

    except Exception as e:
        logger.error(f"Error in IMO25 verification: {e}")
        return {
            "judge_response": f"Error: {str(e)}",
            "correctness_check": "error",
            "is_correct": False,
            "bug_report": f"Verification error: {str(e)}",
            "correctness_score": 0.0,
            "completeness_score": 0.0,
            "has_key_insights": False,
            "errors_found": [f"Judge error: {str(e)}"],
            "overall_assessment": "error",
            "judge_reasoning": "",
            "success": False
        }


def verify_problem_specific_insights(problem_data: Dict, solution: str) -> Dict[str, any]:
    """
    Check for problem-specific insights using our enhanced verification system
    """
    problem_id = problem_data["id"]

    # Use the enhanced verification from our reference module
    insight_verification = verify_key_insights(problem_id, solution)

    return {
        "required_insights_found": len(insight_verification["insights_found"]),
        "total_required_insights": insight_verification["total_insights"],
        "specific_insights": insight_verification["insights_found"],
        "missing_insights": insight_verification["insights_missing"],
        "insight_score": insight_verification["insight_score"]
    }


def extract_solution_quality(response: str) -> Dict[str, any]:
    """
    Analyze the quality of an IMO solution based on mathematical rigor criteria
    """
    analysis = {
        "has_proof_structure": False,
        "uses_mathematical_notation": False,
        "has_logical_steps": False,
        "addresses_all_cases": False,
        "has_conclusion": False,
        "length_score": 0,
        "rigor_indicators": [],
        "completeness_score": 0
    }

    if not response:
        return analysis

    response_lower = response.lower()

    # Check for proof structure
    proof_keywords = ["proof:", "solution:", "we prove", "to show", "suppose", "assume", "let", "consider"]
    if any(keyword in response_lower for keyword in proof_keywords):
        analysis["has_proof_structure"] = True
        analysis["rigor_indicators"].append("proof_structure")

    # Check for mathematical notation
    math_patterns = [r'\$.*\$', r'\\[a-zA-Z]+', r'\\geq', r'\\leq', r'\\in', r'\\mathbb', r'\\sum', r'\\prod']
    if any(re.search(pattern, response) for pattern in math_patterns):
        analysis["uses_mathematical_notation"] = True
        analysis["rigor_indicators"].append("mathematical_notation")

    # Check for logical flow
    logical_words = ["therefore", "thus", "hence", "consequently", "since", "because", "implies", "follows"]
    logical_count = sum(1 for word in logical_words if word in response_lower)
    if logical_count >= 3:
        analysis["has_logical_steps"] = True
        analysis["rigor_indicators"].append("logical_flow")

    # Check for case analysis
    case_words = ["case", "cases", "if", "suppose", "when", "consider"]
    case_count = sum(1 for word in case_words if word in response_lower)
    if case_count >= 2:
        analysis["addresses_all_cases"] = True
        analysis["rigor_indicators"].append("case_analysis")

    # Check for conclusion
    conclusion_words = ["conclude", "final answer", "solution is", "answer:", "qed", "proven", "shown"]
    if any(word in response_lower for word in conclusion_words):
        analysis["has_conclusion"] = True
        analysis["rigor_indicators"].append("clear_conclusion")

    # Length scoring (longer solutions often more complete for IMO)
    word_count = len(response.split())
    if word_count >= 500:
        analysis["length_score"] = 3
    elif word_count >= 200:
        analysis["length_score"] = 2
    elif word_count >= 100:
        analysis["length_score"] = 1
    else:
        analysis["length_score"] = 0

    # Calculate completeness score
    completeness_factors = [
        analysis["has_proof_structure"],
        analysis["uses_mathematical_notation"],
        analysis["has_logical_steps"],
        analysis["addresses_all_cases"],
        analysis["has_conclusion"]
    ]
    analysis["completeness_score"] = sum(completeness_factors) / len(completeness_factors)

    return analysis

def get_llm_response(problem: str, model: str, extra_body: dict = None, timeout: int = 600) -> Dict[str, any]:
    """
    Get response from the LLM for an IMO problem with extended timeout for complex reasoning
    """
    try:
        kwargs = {}
        if extra_body:
            kwargs["extra_body"] = extra_body

        response = client.with_options(timeout=timeout).chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": problem}
            ],
            max_tokens=30000,  # Extended token limit for complex proofs
            **kwargs
        )

        solution_text = response.choices[0].message.content.strip()
        reasoning_tokens = getattr(response.usage, 'reasoning_tokens', 0)
        total_tokens = response.usage.total_tokens if hasattr(response.usage, 'total_tokens') else 0

        return {
            "solution": solution_text,
            "reasoning_tokens": reasoning_tokens,
            "total_tokens": total_tokens,
            "success": True
        }

    except Exception as e:
        logger.error(f"Error getting LLM response: {e}")
        return {
            "solution": f"Error generating solution: {str(e)}",
            "reasoning_tokens": 0,
            "total_tokens": 0,
            "success": False
        }

def evaluate_solution(problem_data: Dict, solution: str, model: str = "google/gemini-2.5-flash-lite") -> Dict[str, any]:
    """
    IMO25-style evaluation using rigorous two-stage verification system:
    1. Detailed verification with comprehensive IMO grader prompt
    2. Simple yes/no check on solution correctness

    This eliminates self-judgment bias and provides more accurate assessment
    """
    logger.info(f"Running IMO25-style evaluation for problem {problem_data['id']}")

    # Use IMO25's rigorous two-stage verification
    imo25_verification = imo25_verify_solution(problem_data["problem"], solution, model)

    # Extract answer for compatibility with existing code
    answer_extraction = extract_final_answer(solution, problem_data["id"])

    # Simple structural analysis for quality metrics
    quality_analysis = extract_solution_quality(solution)

    # In IMO25 system, correctness is binary based on verification
    correctness_score = 1.0 if imo25_verification["is_correct"] else 0.0

    # Confidence based on verification success and quality
    if imo25_verification["is_correct"] and quality_analysis["completeness_score"] > 0.7:
        confidence = "high"
    elif imo25_verification["is_correct"]:
        confidence = "medium"
    else:
        confidence = "low"

    return {
        # Primary binary result - this is what matters
        "is_correct": imo25_verification["is_correct"],
        "verdict": "Correct" if imo25_verification["is_correct"] else "Incorrect",

        # For compatibility with existing analysis code
        "correctness_score": correctness_score,
        "is_likely_correct": imo25_verification["is_correct"],
        "confidence": confidence,

        # Verification details for transparency
        "verification_details": {
            "stage1_analysis": imo25_verification["judge_response"],
            "stage2_check": imo25_verification["correctness_check"],
            "errors_found": imo25_verification["errors_found"],
            "bug_report": imo25_verification["bug_report"] if imo25_verification["bug_report"] else None
        },

        # Legacy compatibility for existing analysis code
        "layer_scores": {
            "structural_quality": quality_analysis["completeness_score"],
            "insights_verification": 1.0 if imo25_verification["is_correct"] else 0.0,
            "llm_judge": correctness_score,
            "answer_extraction": answer_extraction["confidence"]
        },
        "weights_used": {
            "imo25_verification": 1.0  # Single source of truth
        },
        "score_variance": 0.0,  # No variance in binary assessment

        # Simplified component results
        "quality_analysis": quality_analysis,
        "insights_check": {
            "required_insights_found": 1 if imo25_verification["is_correct"] else 0,
            "total_required_insights": 1,
            "insight_score": 1.0 if imo25_verification["is_correct"] else 0.0
        },
        "llm_verification": imo25_verification,
        "answer_extraction": answer_extraction,

        # Method identifier
        "evaluation_method": "imo25_two_stage_binary"
    }

def save_result(filename: str, result: Dict):
    """Save a single result to the results file."""
    results = []
    if os.path.exists(filename):
        try:
            with open(filename, 'r') as f:
                results = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            results = []

    results.append(result)

    with open(filename, 'w') as f:
        json.dump(results, f, indent=2)

def load_existing_results(filename: str) -> List[Dict]:
    """Load existing results from file if it exists."""
    try:
        with open(filename, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        return []

def analyze_results(results: List[Dict], approach_name: str = None):
    """Analyze and print comprehensive statistics of IMO evaluation results"""
    if not results:
        print("No results to analyze")
        return

    total_problems = len(results)
    likely_correct = sum(1 for r in results if r['evaluation']['is_correct'])
    high_confidence = sum(1 for r in results if r['evaluation']['confidence'] == 'high')

    avg_correctness = sum(r['evaluation']['correctness_score'] for r in results) / total_problems
    avg_completeness = sum(r['evaluation']['quality_analysis']['completeness_score'] for r in results) / total_problems

    total_reasoning_tokens = sum(r['response']['reasoning_tokens'] for r in results)
    avg_reasoning_tokens = total_reasoning_tokens / total_problems

    print("\n" + "="*80)
    print(f"IMO 2025 Evaluation Results - {approach_name or 'Baseline'}")
    print("="*80)
    print(f"Total problems attempted: {total_problems}")
    print(f"Likely correct solutions: {likely_correct} ({likely_correct/total_problems:.1%})")
    print(f"High confidence solutions: {high_confidence} ({high_confidence/total_problems:.1%})")
    print(f"Average correctness score: {avg_correctness:.3f}")
    print(f"Average completeness score: {avg_completeness:.3f}")
    print(f"Total reasoning tokens used: {total_reasoning_tokens:,}")
    print(f"Average reasoning tokens per problem: {avg_reasoning_tokens:.0f}")

    # Problem type breakdown
    print(f"\nProblem Type Breakdown:")
    type_stats = {}
    for result in results:
        prob_type = result['problem_data']['type']
        if prob_type not in type_stats:
            type_stats[prob_type] = {'total': 0, 'correct': 0, 'scores': []}
        type_stats[prob_type]['total'] += 1
        if result['evaluation']['is_correct']:
            type_stats[prob_type]['correct'] += 1
        type_stats[prob_type]['scores'].append(result['evaluation']['correctness_score'])

    for prob_type, stats in type_stats.items():
        accuracy = stats['correct'] / stats['total']
        avg_score = sum(stats['scores']) / len(stats['scores'])
        print(f"  {prob_type}: {stats['correct']}/{stats['total']} ({accuracy:.1%}) - Avg score: {avg_score:.3f}")

    # Detailed problem results
    print(f"\nDetailed Results:")
    print("-" * 80)
    for result in results:
        prob_id = result['problem_data']['id']
        prob_type = result['problem_data']['type']
        tokens = result['response']['reasoning_tokens']
        is_correct = result['evaluation']['is_correct']
        verdict = result['evaluation']['verdict']
        status = "✓" if is_correct else "✗"
        print(f"Problem {prob_id} ({prob_type}): {status} {verdict} - {tokens:,} tokens")

    # Quality analysis summary
    print(f"\nSolution Quality Analysis:")
    print("-" * 40)
    quality_metrics = [
        "has_proof_structure", "uses_mathematical_notation", "has_logical_steps",
        "addresses_all_cases", "has_conclusion"
    ]

    for metric in quality_metrics:
        count = sum(1 for r in results if r['evaluation']['quality_analysis'][metric])
        percentage = count / total_problems
        print(f"{metric.replace('_', ' ').title()}: {count}/{total_problems} ({percentage:.1%})")

def main():
    parser = argparse.ArgumentParser(description="Evaluate LLM performance on IMO 2025 problems")
    parser.add_argument("--model", type=str, required=True,
                       help="Model to use (e.g., google/gemma-2.5-flash-lite)")
    parser.add_argument("--approach", type=str, default="none",
                       help="OptiLLM approach to use (none, mars, moa, bon, etc.)")
    parser.add_argument("--timeout", type=int, default=600,
                       help="Timeout in seconds for each problem (default: 600)")
    parser.add_argument("--problems", type=str,
                       help="Comma-separated list of problem IDs to evaluate (e.g., '1,3,5')")

    args = parser.parse_args()

    # Setup results directory and filename
    os.makedirs("results", exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"results/imo25_{args.model.replace('/', '_')}_{args.approach}_{timestamp}.json"

    # Determine which problems to evaluate
    if args.problems:
        problem_ids = [int(x.strip()) for x in args.problems.split(',')]
        problems_to_evaluate = [p for p in IMO_2025_PROBLEMS if p['id'] in problem_ids]
    else:
        problems_to_evaluate = IMO_2025_PROBLEMS

    print(f"Evaluating {len(problems_to_evaluate)} IMO 2025 problems")
    print(f"Model: {args.model}")
    print(f"Approach: {args.approach}")
    print(f"Results will be saved to: {results_file}")

    # Prepare extra_body for approach
    extra_body = {"optillm_approach": args.approach} if args.approach != "none" else None

    # Evaluate each problem
    for problem_data in tqdm(problems_to_evaluate, desc="Solving IMO problems"):
        logger.info(f"Evaluating Problem {problem_data['id']}: {problem_data['type']}")

        start_time = time.time()

        # Get LLM response
        response = get_llm_response(
            problem_data['problem'],
            args.model,
            extra_body,
            args.timeout
        )

        solve_time = time.time() - start_time

        # Evaluate solution quality with enhanced multi-layer approach
        evaluation = evaluate_solution(problem_data, response['solution'], args.model)

        # Compile result
        result = {
            "timestamp": datetime.now().isoformat(),
            "model": args.model,
            "approach": args.approach,
            "problem_data": problem_data,
            "response": response,
            "evaluation": evaluation,
            "solve_time_seconds": solve_time
        }

        # Save result immediately
        save_result(results_file, result)

        logger.info(f"Problem {problem_data['id']} completed - Score: {evaluation['correctness_score']:.3f}")

    # Load all results and analyze
    final_results = load_existing_results(results_file)
    analyze_results(final_results, args.approach)

    print(f"\nEvaluation complete! Results saved to: {results_file}")

if __name__ == "__main__":
    main()